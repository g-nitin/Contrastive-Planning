{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-26T02:50:46.400899Z",
     "start_time": "2024-07-26T02:50:46.387849Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T03:03:56.364369Z",
     "start_time": "2024-07-26T03:03:56.352659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"\n",
    "I have a sokoban problem with the following initial and goal states expressed in PDDL:\n",
    "\n",
    "**Initial and Goal States:**\n",
    "```\n",
    "(define (problem sokoban)\n",
    "(:domain sokoban)\n",
    "(:objects sokoban crate1 crate2 l10 l11 l12 l19 l20 l21 l22 l29 l30 l31 l40 l41 l42 l43 l48 l49 l50 l52 l57 l58 l59 l60 l61)\n",
    "(:init (sokoban sokoban)\n",
    "\t(crate crate1)\n",
    "\t(crate crate2)\n",
    "\t(leftOf l10 l11)\n",
    "\t(leftOf l11 l12)\n",
    "\t(leftOf l19 l20)\n",
    "\t(leftOf l20 l21)\n",
    "\t(leftOf l21 l22)\n",
    "\t(leftOf l29 l30)\n",
    "\t(leftOf l30 l31)\n",
    "\t(leftOf l40 l41)\n",
    "\t(leftOf l41 l42)\n",
    "\t(leftOf l42 l43)\n",
    "\t(leftOf l48 l49)\n",
    "\t(leftOf l49 l50)\n",
    "\t(leftOf l57 l58)\n",
    "\t(leftOf l58 l59)\n",
    "\t(leftOf l59 l60)\n",
    "\t(leftOf l60 l61)\n",
    "\t(below l19 l10)\n",
    "\t(below l20 l11)\n",
    "\t(below l21 l12)\n",
    "\t(below l29 l20)\n",
    "\t(below l30 l21)\n",
    "\t(below l31 l22)\n",
    "\t(below l40 l31)\n",
    "\t(below l49 l40)\n",
    "\t(below l50 l41)\n",
    "\t(below l52 l43)\n",
    "\t(below l57 l48)\n",
    "\t(below l58 l49)\n",
    "\t(below l59 l50)\n",
    "\t(below l61 l52)\n",
    "\t(at sokoban l40)\n",
    "\t(at crate1 l20)\n",
    "\t(at crate2 l30)\n",
    "\t(clear l10)\n",
    "\t(clear l11)\n",
    "\t(clear l12)\n",
    "\t(clear l19)\n",
    "\t(clear l21)\n",
    "\t(clear l22)\n",
    "\t(clear l29)\n",
    "\t(clear l31)\n",
    "\t(clear l41)\n",
    "\t(clear l42)\n",
    "\t(clear l43)\n",
    "\t(clear l48)\n",
    "\t(clear l49)\n",
    "\t(clear l50)\n",
    "\t(clear l52)\n",
    "\t(clear l57)\n",
    "\t(clear l58)\n",
    "\t(clear l59)\n",
    "\t(clear l60)\n",
    "\t(clear l61)\n",
    ")\n",
    "(:goal (and\n",
    "\t(or (at crate1 l41) (at crate2 l41) )\n",
    "\t(or (at crate1 l50) (at crate2 l50) )\n",
    "))\n",
    ")\n",
    "```\n",
    "\n",
    "**Generated Solution Plan:**\n",
    "```\n",
    "\t(moveup sokoban l40 l31)\n",
    "(moveup sokoban l31 l22)\n",
    "(moveleft sokoban l22 l21)\n",
    "(moveup sokoban l21 l12)\n",
    "(moveleft sokoban l12 l11)\n",
    "(moveleft sokoban l11 l10)\n",
    "(movedown sokoban l10 l19)\n",
    "(pushright sokoban l19 l20 l21 crate1)\n",
    "(movedown sokoban l20 l29)\n",
    "(pushright sokoban l29 l30 l31 crate2)\n",
    "(moveleft sokoban l30 l29)\n",
    "(moveup sokoban l29 l20)\n",
    "(moveup sokoban l20 l11)\n",
    "(moveright sokoban l11 l12)\n",
    "(pushdown sokoban l12 l21 l30 crate1)\n",
    "(moveright sokoban l21 l22)\n",
    "(pushdown sokoban l22 l31 l40 crate2)\n",
    "(pushdown sokoban l31 l40 l49 crate2)\n",
    "(moveright sokoban l40 l41)\n",
    "(movedown sokoban l41 l50)\n",
    "(movedown sokoban l50 l59)\n",
    "(moveleft sokoban l59 l58)\n",
    "(moveleft sokoban l58 l57)\n",
    "(moveup sokoban l57 l48)\n",
    "(pushright sokoban l48 l49 l50 crate2)\n",
    "(moveup sokoban l49 l40)\n",
    "(moveup sokoban l40 l31)\n",
    "(moveup sokoban l31 l22)\n",
    "(moveleft sokoban l22 l21)\n",
    "(moveleft sokoban l21 l20)\n",
    "(movedown sokoban l20 l29)\n",
    "(pushright sokoban l29 l30 l31 crate1)\n",
    "(moveup sokoban l30 l21)\n",
    "(moveright sokoban l21 l22)\n",
    "(pushdown sokoban l22 l31 l40 crate1)\n",
    "(pushdown sokoban l31 l40 l49 crate1)\n",
    "(moveright sokoban l40 l41)\n",
    "(moveright sokoban l41 l42)\n",
    "(moveright sokoban l42 l43)\n",
    "(movedown sokoban l43 l52)\n",
    "(movedown sokoban l52 l61)\n",
    "(moveleft sokoban l61 l60)\n",
    "(moveleft sokoban l60 l59)\n",
    "(pushup sokoban l59 l50 l41 crate2)\n",
    "(movedown sokoban l50 l59)\n",
    "(moveleft sokoban l59 l58)\n",
    "(moveleft sokoban l58 l57)\n",
    "(moveup sokoban l57 l48)\n",
    "(pushright sokoban l48 l49 l50 crate1)\n",
    "```\n",
    "\n",
    "I need you to answer the following question in under 100 words by reasoning through the provided information:\n",
    "\n",
    "**Question:**\n",
    "```\n",
    "Is the plan, provided above, valid?\n",
    "```\n",
    "\n",
    "Hints:\n",
    "    There are 8 actions for the Sokoban domain.\n",
    "    The actions with the format of `move* ?x ?y` are the action that move the Sokoban from location `?x` to `?y`.\n",
    "    The actions with the format of `push* ?x ?y ?z ?crate` are the action that push the Sokoban from `?x` to `?y` and `?crate` from location `?y` to `?z`.\n",
    "\"\"\""
   ],
   "id": "b18dda8f8a7b53fa",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T03:08:18.266151Z",
     "start_time": "2024-07-26T03:08:17.981209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "    token=getenv(\"HF_API_KEY\"),\n",
    ")\n",
    "\n",
    "for message in client.chat_completion(\n",
    "\tmessages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "\tmax_tokens=500,\n",
    "\tstream=False,\n",
    "):\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ],
   "id": "ae1fbc6dbedc2cbd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Server https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions does not seem to support chat completion. Falling back to text generation. Error:  (Request ID: awyQhAglf1l7y09Wq0DA0)\n",
      "\n",
      "Bad request:\n",
      "Model requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": " (Request ID: HsZgPumu2-3NrBGQp_XIJ)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:304\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 304\u001B[0m     response\u001B[38;5;241m.\u001B[39mraise_for_status()\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/requests/models.py:1021\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1020\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[0;32m-> 1021\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mHTTPError\u001B[0m: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mBadRequestError\u001B[0m                           Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:706\u001B[0m, in \u001B[0;36mInferenceClient.chat_completion\u001B[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p)\u001B[0m\n\u001B[1;32m    705\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 706\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpost(\n\u001B[1;32m    707\u001B[0m         model\u001B[38;5;241m=\u001B[39mmodel_url,\n\u001B[1;32m    708\u001B[0m         json\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(\n\u001B[1;32m    709\u001B[0m             model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtgi\u001B[39m\u001B[38;5;124m\"\u001B[39m,  \u001B[38;5;66;03m# random string\u001B[39;00m\n\u001B[1;32m    710\u001B[0m             messages\u001B[38;5;241m=\u001B[39mmessages,\n\u001B[1;32m    711\u001B[0m             frequency_penalty\u001B[38;5;241m=\u001B[39mfrequency_penalty,\n\u001B[1;32m    712\u001B[0m             logit_bias\u001B[38;5;241m=\u001B[39mlogit_bias,\n\u001B[1;32m    713\u001B[0m             logprobs\u001B[38;5;241m=\u001B[39mlogprobs,\n\u001B[1;32m    714\u001B[0m             max_tokens\u001B[38;5;241m=\u001B[39mmax_tokens,\n\u001B[1;32m    715\u001B[0m             n\u001B[38;5;241m=\u001B[39mn,\n\u001B[1;32m    716\u001B[0m             presence_penalty\u001B[38;5;241m=\u001B[39mpresence_penalty,\n\u001B[1;32m    717\u001B[0m             seed\u001B[38;5;241m=\u001B[39mseed,\n\u001B[1;32m    718\u001B[0m             stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[1;32m    719\u001B[0m             temperature\u001B[38;5;241m=\u001B[39mtemperature,\n\u001B[1;32m    720\u001B[0m             tool_choice\u001B[38;5;241m=\u001B[39mtool_choice,\n\u001B[1;32m    721\u001B[0m             tool_prompt\u001B[38;5;241m=\u001B[39mtool_prompt,\n\u001B[1;32m    722\u001B[0m             tools\u001B[38;5;241m=\u001B[39mtools,\n\u001B[1;32m    723\u001B[0m             top_logprobs\u001B[38;5;241m=\u001B[39mtop_logprobs,\n\u001B[1;32m    724\u001B[0m             top_p\u001B[38;5;241m=\u001B[39mtop_p,\n\u001B[1;32m    725\u001B[0m             stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m    726\u001B[0m         ),\n\u001B[1;32m    727\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m    728\u001B[0m     )\n\u001B[1;32m    729\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:273\u001B[0m, in \u001B[0;36mInferenceClient.post\u001B[0;34m(self, json, data, model, task, stream)\u001B[0m\n\u001B[1;32m    272\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 273\u001B[0m     hf_raise_for_status(response)\n\u001B[1;32m    274\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\u001B[38;5;241m.\u001B[39miter_lines() \u001B[38;5;28;01mif\u001B[39;00m stream \u001B[38;5;28;01melse\u001B[39;00m response\u001B[38;5;241m.\u001B[39mcontent\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:358\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    355\u001B[0m     message \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    356\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mBad request for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mendpoint_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m endpoint:\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m endpoint_name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mBad request:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    357\u001B[0m     )\n\u001B[0;32m--> 358\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m BadRequestError(message, response\u001B[38;5;241m=\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    360\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m403\u001B[39m:\n",
      "\u001B[0;31mBadRequestError\u001B[0m:  (Request ID: awyQhAglf1l7y09Wq0DA0)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:304\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 304\u001B[0m     response\u001B[38;5;241m.\u001B[39mraise_for_status()\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/requests/models.py:1021\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1020\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[0;32m-> 1021\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mHTTPError\u001B[0m: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-70B-Instruct",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mBadRequestError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 8\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhuggingface_hub\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m InferenceClient\n\u001B[1;32m      3\u001B[0m client \u001B[38;5;241m=\u001B[39m InferenceClient(\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmeta-llama/Meta-Llama-3.1-70B-Instruct\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      5\u001B[0m     token\u001B[38;5;241m=\u001B[39mgetenv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHF_API_KEY\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m      6\u001B[0m )\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m message \u001B[38;5;129;01min\u001B[39;00m client\u001B[38;5;241m.\u001B[39mchat_completion(\n\u001B[1;32m      9\u001B[0m \tmessages\u001B[38;5;241m=\u001B[39m[{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m: prompt}],\n\u001B[1;32m     10\u001B[0m \tmax_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m500\u001B[39m,\n\u001B[1;32m     11\u001B[0m \tstream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     12\u001B[0m ):\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28mprint\u001B[39m(message\u001B[38;5;241m.\u001B[39mchoices[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mdelta\u001B[38;5;241m.\u001B[39mcontent, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:738\u001B[0m, in \u001B[0;36mInferenceClient.chat_completion\u001B[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p)\u001B[0m\n\u001B[1;32m    734\u001B[0m         _set_as_non_chat_completion_server(model)\n\u001B[1;32m    735\u001B[0m         logger\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[1;32m    736\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mServer \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_url\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not seem to support chat completion. Falling back to text generation. Error: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    737\u001B[0m         )\n\u001B[0;32m--> 738\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchat_completion(\n\u001B[1;32m    739\u001B[0m             messages\u001B[38;5;241m=\u001B[39mmessages,\n\u001B[1;32m    740\u001B[0m             model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m    741\u001B[0m             stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m    742\u001B[0m             max_tokens\u001B[38;5;241m=\u001B[39mmax_tokens,\n\u001B[1;32m    743\u001B[0m             seed\u001B[38;5;241m=\u001B[39mseed,\n\u001B[1;32m    744\u001B[0m             stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[1;32m    745\u001B[0m             temperature\u001B[38;5;241m=\u001B[39mtemperature,\n\u001B[1;32m    746\u001B[0m             top_p\u001B[38;5;241m=\u001B[39mtop_p,\n\u001B[1;32m    747\u001B[0m         )\n\u001B[1;32m    748\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[1;32m    750\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream:\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:770\u001B[0m, in \u001B[0;36mInferenceClient.chat_completion\u001B[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p)\u001B[0m\n\u001B[1;32m    764\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    765\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTools are not supported by the model. This is due to the model not been served by a \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    766\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mText-Generation-Inference server. The provided tool parameters will be ignored.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    767\u001B[0m     )\n\u001B[1;32m    769\u001B[0m \u001B[38;5;66;03m# generate response\u001B[39;00m\n\u001B[0;32m--> 770\u001B[0m text_generation_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_generation(\n\u001B[1;32m    771\u001B[0m     prompt\u001B[38;5;241m=\u001B[39mmessages,  \u001B[38;5;66;03m# type: ignore # Not correct type but works implicitly\u001B[39;00m\n\u001B[1;32m    772\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m    773\u001B[0m     stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    774\u001B[0m     details\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    775\u001B[0m     max_new_tokens\u001B[38;5;241m=\u001B[39mmax_tokens,\n\u001B[1;32m    776\u001B[0m     seed\u001B[38;5;241m=\u001B[39mseed,\n\u001B[1;32m    777\u001B[0m     stop_sequences\u001B[38;5;241m=\u001B[39mstop,\n\u001B[1;32m    778\u001B[0m     temperature\u001B[38;5;241m=\u001B[39mtemperature,\n\u001B[1;32m    779\u001B[0m     top_p\u001B[38;5;241m=\u001B[39mtop_p,\n\u001B[1;32m    780\u001B[0m )\n\u001B[1;32m    782\u001B[0m \u001B[38;5;66;03m# Format as a ChatCompletionOutput with dummy values for fields we can't provide\u001B[39;00m\n\u001B[1;32m    783\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ChatCompletionOutput(\n\u001B[1;32m    784\u001B[0m     \u001B[38;5;28mid\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdummy\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    785\u001B[0m     model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdummy\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    799\u001B[0m     ],\n\u001B[1;32m    800\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:2060\u001B[0m, in \u001B[0;36mInferenceClient.text_generation\u001B[0;34m(self, prompt, details, stream, model, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001B[0m\n\u001B[1;32m   2036\u001B[0m         _set_unsupported_text_generation_kwargs(model, unused_params)\n\u001B[1;32m   2037\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_generation(  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m   2038\u001B[0m             prompt\u001B[38;5;241m=\u001B[39mprompt,\n\u001B[1;32m   2039\u001B[0m             details\u001B[38;5;241m=\u001B[39mdetails,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2058\u001B[0m             watermark\u001B[38;5;241m=\u001B[39mwatermark,\n\u001B[1;32m   2059\u001B[0m         )\n\u001B[0;32m-> 2060\u001B[0m     raise_text_generation_error(e)\n\u001B[1;32m   2062\u001B[0m \u001B[38;5;66;03m# Parse output\u001B[39;00m\n\u001B[1;32m   2063\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream:\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/inference/_common.py:460\u001B[0m, in \u001B[0;36mraise_text_generation_error\u001B[0;34m(http_error)\u001B[0m\n\u001B[1;32m    457\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exception \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhttp_error\u001B[39;00m\n\u001B[1;32m    459\u001B[0m \u001B[38;5;66;03m# Otherwise, fallback to default error\u001B[39;00m\n\u001B[0;32m--> 460\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m http_error\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:2031\u001B[0m, in \u001B[0;36mInferenceClient.text_generation\u001B[0;34m(self, prompt, details, stream, model, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001B[0m\n\u001B[1;32m   2029\u001B[0m \u001B[38;5;66;03m# Handle errors separately for more precise error messages\u001B[39;00m\n\u001B[1;32m   2030\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 2031\u001B[0m     bytes_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpost(json\u001B[38;5;241m=\u001B[39mpayload, model\u001B[38;5;241m=\u001B[39mmodel, task\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext-generation\u001B[39m\u001B[38;5;124m\"\u001B[39m, stream\u001B[38;5;241m=\u001B[39mstream)  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m   2032\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   2033\u001B[0m     match \u001B[38;5;241m=\u001B[39m MODEL_KWARGS_NOT_USED_REGEX\u001B[38;5;241m.\u001B[39msearch(\u001B[38;5;28mstr\u001B[39m(e))\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:273\u001B[0m, in \u001B[0;36mInferenceClient.post\u001B[0;34m(self, json, data, model, task, stream)\u001B[0m\n\u001B[1;32m    270\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InferenceTimeoutError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInference call timed out: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merror\u001B[39;00m  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m    272\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 273\u001B[0m     hf_raise_for_status(response)\n\u001B[1;32m    274\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\u001B[38;5;241m.\u001B[39miter_lines() \u001B[38;5;28;01mif\u001B[39;00m stream \u001B[38;5;28;01melse\u001B[39;00m response\u001B[38;5;241m.\u001B[39mcontent\n\u001B[1;32m    275\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m error:\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:358\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m400\u001B[39m:\n\u001B[1;32m    355\u001B[0m     message \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    356\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mBad request for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mendpoint_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m endpoint:\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m endpoint_name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mBad request:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    357\u001B[0m     )\n\u001B[0;32m--> 358\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m BadRequestError(message, response\u001B[38;5;241m=\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    360\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m403\u001B[39m:\n\u001B[1;32m    361\u001B[0m     message \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    362\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Forbidden: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00merror_message\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    363\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mCannot access content at: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    364\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mIf you are trying to create or update content,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    365\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmake sure you have a token with the `write` role.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    366\u001B[0m     )\n",
      "\u001B[0;31mBadRequestError\u001B[0m:  (Request ID: HsZgPumu2-3NrBGQp_XIJ)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query."
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T03:05:39.243762Z",
     "start_time": "2024-07-26T03:05:30.911771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])\n"
   ],
   "id": "78ef65be0656bea5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nitingupta/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-66a31282-5ceb72b1385373ea00a42abf;bde813a5-9b3d-4340-aedc-f2b65995bce1)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3.1-8B-Instruct is restricted. You must be authenticated to access it.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:304\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 304\u001B[0m     response\u001B[38;5;241m.\u001B[39mraise_for_status()\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/requests/models.py:1021\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1020\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[0;32m-> 1021\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mHTTPError\u001B[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mGatedRepoError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/transformers/utils/hub.py:399\u001B[0m, in \u001B[0;36mcached_file\u001B[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[1;32m    397\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    398\u001B[0m     \u001B[38;5;66;03m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[0;32m--> 399\u001B[0m     resolved_file \u001B[38;5;241m=\u001B[39m hf_hub_download(\n\u001B[1;32m    400\u001B[0m         path_or_repo_id,\n\u001B[1;32m    401\u001B[0m         filename,\n\u001B[1;32m    402\u001B[0m         subfolder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(subfolder) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m subfolder,\n\u001B[1;32m    403\u001B[0m         repo_type\u001B[38;5;241m=\u001B[39mrepo_type,\n\u001B[1;32m    404\u001B[0m         revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[1;32m    405\u001B[0m         cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[1;32m    406\u001B[0m         user_agent\u001B[38;5;241m=\u001B[39muser_agent,\n\u001B[1;32m    407\u001B[0m         force_download\u001B[38;5;241m=\u001B[39mforce_download,\n\u001B[1;32m    408\u001B[0m         proxies\u001B[38;5;241m=\u001B[39mproxies,\n\u001B[1;32m    409\u001B[0m         resume_download\u001B[38;5;241m=\u001B[39mresume_download,\n\u001B[1;32m    410\u001B[0m         token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[1;32m    411\u001B[0m         local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[1;32m    412\u001B[0m     )\n\u001B[1;32m    413\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m GatedRepoError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[0;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/file_download.py:1221\u001B[0m, in \u001B[0;36mhf_hub_download\u001B[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001B[0m\n\u001B[1;32m   1220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _hf_hub_download_to_cache_dir(\n\u001B[1;32m   1222\u001B[0m         \u001B[38;5;66;03m# Destination\u001B[39;00m\n\u001B[1;32m   1223\u001B[0m         cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[1;32m   1224\u001B[0m         \u001B[38;5;66;03m# File info\u001B[39;00m\n\u001B[1;32m   1225\u001B[0m         repo_id\u001B[38;5;241m=\u001B[39mrepo_id,\n\u001B[1;32m   1226\u001B[0m         filename\u001B[38;5;241m=\u001B[39mfilename,\n\u001B[1;32m   1227\u001B[0m         repo_type\u001B[38;5;241m=\u001B[39mrepo_type,\n\u001B[1;32m   1228\u001B[0m         revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[1;32m   1229\u001B[0m         \u001B[38;5;66;03m# HTTP info\u001B[39;00m\n\u001B[1;32m   1230\u001B[0m         headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[1;32m   1231\u001B[0m         proxies\u001B[38;5;241m=\u001B[39mproxies,\n\u001B[1;32m   1232\u001B[0m         etag_timeout\u001B[38;5;241m=\u001B[39metag_timeout,\n\u001B[1;32m   1233\u001B[0m         endpoint\u001B[38;5;241m=\u001B[39mendpoint,\n\u001B[1;32m   1234\u001B[0m         \u001B[38;5;66;03m# Additional options\u001B[39;00m\n\u001B[1;32m   1235\u001B[0m         local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[1;32m   1236\u001B[0m         force_download\u001B[38;5;241m=\u001B[39mforce_download,\n\u001B[1;32m   1237\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/file_download.py:1325\u001B[0m, in \u001B[0;36m_hf_hub_download_to_cache_dir\u001B[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001B[0m\n\u001B[1;32m   1324\u001B[0m     \u001B[38;5;66;03m# Otherwise, raise appropriate error\u001B[39;00m\n\u001B[0;32m-> 1325\u001B[0m     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n\u001B[1;32m   1327\u001B[0m \u001B[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/file_download.py:1823\u001B[0m, in \u001B[0;36m_raise_on_head_call_error\u001B[0;34m(head_call_error, force_download, local_files_only)\u001B[0m\n\u001B[1;32m   1821\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(head_call_error, RepositoryNotFoundError) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(head_call_error, GatedRepoError):\n\u001B[1;32m   1822\u001B[0m     \u001B[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001B[39;00m\n\u001B[0;32m-> 1823\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m head_call_error\n\u001B[1;32m   1824\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1825\u001B[0m     \u001B[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/file_download.py:1722\u001B[0m, in \u001B[0;36m_get_metadata_or_catch_error\u001B[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, local_files_only, relative_filename, storage_folder)\u001B[0m\n\u001B[1;32m   1721\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1722\u001B[0m     metadata \u001B[38;5;241m=\u001B[39m get_hf_file_metadata(url\u001B[38;5;241m=\u001B[39murl, proxies\u001B[38;5;241m=\u001B[39mproxies, timeout\u001B[38;5;241m=\u001B[39metag_timeout, headers\u001B[38;5;241m=\u001B[39mheaders)\n\u001B[1;32m   1723\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m EntryNotFoundError \u001B[38;5;28;01mas\u001B[39;00m http_error:\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[0;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/file_download.py:1645\u001B[0m, in \u001B[0;36mget_hf_file_metadata\u001B[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001B[0m\n\u001B[1;32m   1644\u001B[0m \u001B[38;5;66;03m# Retrieve metadata\u001B[39;00m\n\u001B[0;32m-> 1645\u001B[0m r \u001B[38;5;241m=\u001B[39m _request_wrapper(\n\u001B[1;32m   1646\u001B[0m     method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHEAD\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1647\u001B[0m     url\u001B[38;5;241m=\u001B[39murl,\n\u001B[1;32m   1648\u001B[0m     headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[1;32m   1649\u001B[0m     allow_redirects\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   1650\u001B[0m     follow_relative_redirects\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m   1651\u001B[0m     proxies\u001B[38;5;241m=\u001B[39mproxies,\n\u001B[1;32m   1652\u001B[0m     timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[1;32m   1653\u001B[0m )\n\u001B[1;32m   1654\u001B[0m hf_raise_for_status(r)\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/file_download.py:372\u001B[0m, in \u001B[0;36m_request_wrapper\u001B[0;34m(method, url, follow_relative_redirects, **params)\u001B[0m\n\u001B[1;32m    371\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m follow_relative_redirects:\n\u001B[0;32m--> 372\u001B[0m     response \u001B[38;5;241m=\u001B[39m _request_wrapper(\n\u001B[1;32m    373\u001B[0m         method\u001B[38;5;241m=\u001B[39mmethod,\n\u001B[1;32m    374\u001B[0m         url\u001B[38;5;241m=\u001B[39murl,\n\u001B[1;32m    375\u001B[0m         follow_relative_redirects\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    376\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams,\n\u001B[1;32m    377\u001B[0m     )\n\u001B[1;32m    379\u001B[0m     \u001B[38;5;66;03m# If redirection, we redirect only relative paths.\u001B[39;00m\n\u001B[1;32m    380\u001B[0m     \u001B[38;5;66;03m# This is useful in case of a renamed repository.\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/file_download.py:396\u001B[0m, in \u001B[0;36m_request_wrapper\u001B[0;34m(method, url, follow_relative_redirects, **params)\u001B[0m\n\u001B[1;32m    395\u001B[0m response \u001B[38;5;241m=\u001B[39m get_session()\u001B[38;5;241m.\u001B[39mrequest(method\u001B[38;5;241m=\u001B[39mmethod, url\u001B[38;5;241m=\u001B[39murl, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n\u001B[0;32m--> 396\u001B[0m hf_raise_for_status(response)\n\u001B[1;32m    397\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:321\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    318\u001B[0m     message \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    319\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Client Error.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot access gated repo for url \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    320\u001B[0m     )\n\u001B[0;32m--> 321\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m GatedRepoError(message, response) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    323\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m error_message \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccess to this resource is disabled.\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[0;31mGatedRepoError\u001B[0m: 401 Client Error. (Request ID: Root=1-66a31282-5ceb72b1385373ea00a42abf;bde813a5-9b3d-4340-aedc-f2b65995bce1)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3.1-8B-Instruct is restricted. You must be authenticated to access it.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 6\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m      4\u001B[0m model_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmeta-llama/Meta-Llama-3.1-8B-Instruct\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 6\u001B[0m pipeline \u001B[38;5;241m=\u001B[39m transformers\u001B[38;5;241m.\u001B[39mpipeline(\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext-generation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      8\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel_id,\n\u001B[1;32m      9\u001B[0m     model_kwargs\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch_dtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: torch\u001B[38;5;241m.\u001B[39mbfloat16},\n\u001B[1;32m     10\u001B[0m     device_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     11\u001B[0m )\n\u001B[1;32m     13\u001B[0m messages \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     14\u001B[0m     {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msystem\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are a pirate chatbot who always responds in pirate speak!\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[1;32m     15\u001B[0m     {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWho are you?\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[1;32m     16\u001B[0m ]\n\u001B[1;32m     18\u001B[0m outputs \u001B[38;5;241m=\u001B[39m pipeline(\n\u001B[1;32m     19\u001B[0m     messages,\n\u001B[1;32m     20\u001B[0m     max_new_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m,\n\u001B[1;32m     21\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/transformers/pipelines/__init__.py:816\u001B[0m, in \u001B[0;36mpipeline\u001B[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001B[0m\n\u001B[1;32m    813\u001B[0m                 adapter_config \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(f)\n\u001B[1;32m    814\u001B[0m                 model \u001B[38;5;241m=\u001B[39m adapter_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbase_model_name_or_path\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m--> 816\u001B[0m     config \u001B[38;5;241m=\u001B[39m AutoConfig\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m    817\u001B[0m         model, _from_pipeline\u001B[38;5;241m=\u001B[39mtask, code_revision\u001B[38;5;241m=\u001B[39mcode_revision, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs\n\u001B[1;32m    818\u001B[0m     )\n\u001B[1;32m    819\u001B[0m     hub_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39m_commit_hash\n\u001B[1;32m    821\u001B[0m custom_tasks \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:934\u001B[0m, in \u001B[0;36mAutoConfig.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    931\u001B[0m trust_remote_code \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrust_remote_code\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    932\u001B[0m code_revision \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcode_revision\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 934\u001B[0m config_dict, unused_kwargs \u001B[38;5;241m=\u001B[39m PretrainedConfig\u001B[38;5;241m.\u001B[39mget_config_dict(pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    935\u001B[0m has_remote_code \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto_map\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAutoConfig\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto_map\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    936\u001B[0m has_local_code \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_type\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict \u001B[38;5;129;01mand\u001B[39;00m config_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_type\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;129;01min\u001B[39;00m CONFIG_MAPPING\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/transformers/configuration_utils.py:632\u001B[0m, in \u001B[0;36mPretrainedConfig.get_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    630\u001B[0m original_kwargs \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(kwargs)\n\u001B[1;32m    631\u001B[0m \u001B[38;5;66;03m# Get config dict associated with the base config file\u001B[39;00m\n\u001B[0;32m--> 632\u001B[0m config_dict, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_get_config_dict(pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict:\n\u001B[1;32m    634\u001B[0m     original_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m config_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/transformers/configuration_utils.py:689\u001B[0m, in \u001B[0;36mPretrainedConfig._get_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    685\u001B[0m configuration_file \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_configuration_file\u001B[39m\u001B[38;5;124m\"\u001B[39m, CONFIG_NAME) \u001B[38;5;28;01mif\u001B[39;00m gguf_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m gguf_file\n\u001B[1;32m    687\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    688\u001B[0m     \u001B[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001B[39;00m\n\u001B[0;32m--> 689\u001B[0m     resolved_config_file \u001B[38;5;241m=\u001B[39m cached_file(\n\u001B[1;32m    690\u001B[0m         pretrained_model_name_or_path,\n\u001B[1;32m    691\u001B[0m         configuration_file,\n\u001B[1;32m    692\u001B[0m         cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[1;32m    693\u001B[0m         force_download\u001B[38;5;241m=\u001B[39mforce_download,\n\u001B[1;32m    694\u001B[0m         proxies\u001B[38;5;241m=\u001B[39mproxies,\n\u001B[1;32m    695\u001B[0m         resume_download\u001B[38;5;241m=\u001B[39mresume_download,\n\u001B[1;32m    696\u001B[0m         local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[1;32m    697\u001B[0m         token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[1;32m    698\u001B[0m         user_agent\u001B[38;5;241m=\u001B[39muser_agent,\n\u001B[1;32m    699\u001B[0m         revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[1;32m    700\u001B[0m         subfolder\u001B[38;5;241m=\u001B[39msubfolder,\n\u001B[1;32m    701\u001B[0m         _commit_hash\u001B[38;5;241m=\u001B[39mcommit_hash,\n\u001B[1;32m    702\u001B[0m     )\n\u001B[1;32m    703\u001B[0m     commit_hash \u001B[38;5;241m=\u001B[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m:\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001B[39;00m\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;66;03m# the original exception.\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/auto-plan/lib/python3.11/site-packages/transformers/utils/hub.py:417\u001B[0m, in \u001B[0;36mcached_file\u001B[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[1;32m    415\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m resolved_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _raise_exceptions_for_gated_repo:\n\u001B[1;32m    416\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m resolved_file\n\u001B[0;32m--> 417\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    418\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to access a gated repo.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mMake sure to have access to it at \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    419\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://huggingface.co/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    420\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    421\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RepositoryNotFoundError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    422\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    423\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not a local folder and is not a valid model identifier \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    424\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlisted on \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/models\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mIf this is a private repository, make sure to pass a token \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    425\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    426\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`token=<your_token>`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    427\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[0;31mOSError\u001B[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-66a31282-5ceb72b1385373ea00a42abf;bde813a5-9b3d-4340-aedc-f2b65995bce1)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3.1-8B-Instruct is restricted. You must be authenticated to access it."
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6a4b72e0ebc8f00e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
