{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Zero-Shot Classification\n",
    "\n",
    "In this notebook, we'll use the `zero-shot-classification` pipeline from the HF Transformers library to predict the intents of sentences in a dataset. We'll compare the predicted intents with the actual labels and print the evaluation metrics.\n",
    "\n",
    "Creating an NLP-based framework to parse the input question to categorize the intent into one of the question types.\n",
    "\n",
    "Question Types:\n",
    "1. Why is action A not used in the plan, rather than being used?\n",
    "2. Why is action A used in the plan, rather than not being used?\n",
    "3. Why is action A used in state S, rather than action B?"
   ],
   "id": "65af76665474c488"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Single Text Prediction",
   "id": "3996a58d6e7648eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T01:24:09.764861Z",
     "start_time": "2024-06-03T01:24:07.392205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "from pprint import pprint"
   ],
   "id": "f985e2baedd364d2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-03T01:24:13.481780Z",
     "start_time": "2024-06-03T01:24:09.765715Z"
    }
   },
   "source": [
    "# Load a pre-trained zero-shot classification pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T01:24:15.452639Z",
     "start_time": "2024-06-03T01:24:13.482860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the query and candidate labels\n",
    "candidate_labels = [\"Why is action A not used in the plan?\", \n",
    "                    \"Why is action A used in the plan?\", \n",
    "                    \"Why is action A used in state S, rather than action B?\"]\n",
    "query = \"What made 'push box to the left' more suitable than 'move to the right'?\"\n",
    "\n",
    "# Perform zero-shot classification\n",
    "result = classifier(query, candidate_labels)\n",
    "pprint(result, width=100)"
   ],
   "id": "11ca92d38e459308",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': ['Why is action A used in the plan?',\n",
      "            'Why is action A not used in the plan?',\n",
      "            'Why is action A used in state S, rather than action B?'],\n",
      " 'scores': [0.3973885476589203, 0.3694774806499481, 0.2331339716911316],\n",
      " 'sequence': \"What made 'push box to the left' more suitable than 'move to the right'?\"}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T01:24:17.616969Z",
     "start_time": "2024-06-03T01:24:15.454480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the query and candidate labels\n",
    "candidate_labels = [\"Why is action A not used in the plan?\", \n",
    "                    \"Why is action A used in the plan?\", \n",
    "                    \"Why is action A used rather than action B?\"]\n",
    "query = \"What made 'push box to the left' more suitable than 'move to the right'?\"\n",
    "\n",
    "# Perform zero-shot classification\n",
    "result = classifier(query, candidate_labels)\n",
    "pprint(result, width=100)"
   ],
   "id": "7c4f14fc6d35d73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': ['Why is action A used rather than action B?',\n",
      "            'Why is action A used in the plan?',\n",
      "            'Why is action A not used in the plan?'],\n",
      " 'scores': [0.7602096199989319, 0.12425892055034637, 0.11553144454956055],\n",
      " 'sequence': \"What made 'push box to the left' more suitable than 'move to the right'?\"}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It seems that \"Why is action A used rather than action B?\" is a better intent category label than \"Why is action A used in state S, rather than action B?\".",
   "id": "792b0f39e6f149da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<br>",
   "id": "e4973a09e9b1342d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dataset Prediction using Base Model\n",
    "Predict the intents of the sentences in the text column from the data csv, compare them with the actual labels, and print the evaluation metrics.\n",
    "\n",
    "Use the base model."
   ],
   "id": "9da3c2e820168eb9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T01:53:48.037720Z",
     "start_time": "2024-06-03T01:53:48.033872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import swifter\n",
    "import wandb\n",
    "import os"
   ],
   "id": "eeeacfb419714fb6",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T01:54:25.456006Z",
     "start_time": "2024-06-03T01:54:25.449110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"] = \"zero-shot-classification\"\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\""
   ],
   "id": "95c8aea6223cf625",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T01:55:56.125445Z",
     "start_time": "2024-06-03T01:55:56.052418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('./data/combined_dataset.csv')\n",
    "print(f\"Number of rows in the dataset: {df.shape[0]}\")\n",
    "df.head()"
   ],
   "id": "f63211774ea13f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the dataset: 346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                text  label\n",
       "0           Why was action A excluded from the plan?      1\n",
       "1  What were the reasons for omitting action A fr...      1\n",
       "2  Can you explain why action A was not considere...      1\n",
       "3              Why didn't the plan include action A?      1\n",
       "4  What is the rationale for not using action A i...      1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why was action A excluded from the plan?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What were the reasons for omitting action A fr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can you explain why action A was not considere...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why didn't the plan include action A?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the rationale for not using action A i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T01:56:01.903743Z",
     "start_time": "2024-06-03T01:56:01.898107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the candidate labels and their corresponding intent numbers\n",
    "candidate_labels = [\"Why is action A not used in the plan?\", \n",
    "                    \"Why is action A used in the plan?\", \n",
    "                    \"Why is action A used rather than action B?\"]\n",
    "\n",
    "intent_to_label = {label: intent for label, intent in zip(candidate_labels, range(1, 4))}\n",
    "intent_to_label"
   ],
   "id": "e4aea201d3bdc9ac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Why is action A not used in the plan?': 1,\n",
       " 'Why is action A used in the plan?': 2,\n",
       " 'Why is action A used rather than action B?': 3}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T01:56:08.319812Z",
     "start_time": "2024-06-03T01:56:04.257631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load a pre-trained zero-shot classification pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Function to get predictions for each text\n",
    "def get_prediction(text):\n",
    "    result = classifier(text, candidate_labels)\n",
    "    predicted_label = intent_to_label[result['labels'][0]]\n",
    "    return predicted_label"
   ],
   "id": "70fc8d178444a138",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T02:08:53.570342Z",
     "start_time": "2024-06-03T01:56:08.861162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply the function to the text column\n",
    "df['predicted_label'] = df['text'].swifter.apply(get_prediction)"
   ],
   "id": "9a858916c12682b1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pandas Apply:   0%|          | 0/346 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d40e9a50c21c4e8cbe32aa1211f95d0b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T02:08:53.587383Z",
     "start_time": "2024-06-03T02:08:53.572884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compare predicted labels with actual labels\n",
    "y_true = df['label']\n",
    "y_pred = df['predicted_label']\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_true, y_pred):.2f}\")"
   ],
   "id": "a5c48f13821b6de4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.97      0.84       116\n",
      "           2       0.98      0.94      0.96       115\n",
      "           3       0.90      0.65      0.76       115\n",
      "\n",
      "    accuracy                           0.86       346\n",
      "   macro avg       0.87      0.86      0.85       346\n",
      "weighted avg       0.87      0.86      0.85       346\n",
      "\n",
      "Accuracy: 0.86\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T02:08:53.600333Z",
     "start_time": "2024-06-03T02:08:53.588417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display the rows in which the predictions didn't match the label\n",
    "incorrect_predictions = df[df['label'] != df['predicted_label']]\n",
    "print(f\"{incorrect_predictions.shape[0]} incorrect predictions out of {df.shape[0]} test samples.\")\n",
    "incorrect_predictions.head()"
   ],
   "id": "51ca88d65c800c3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 incorrect predictions out of 346 test samples.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                  text  label  predicted_label\n",
       "46   The player doesn't push any boxes. Shouldn't p...      1                2\n",
       "96   What's the justification for not using 'move r...      1                3\n",
       "102     What is the reasoning for not using 'push up'?      1                3\n",
       "176  What made the plan opt for 'move left' instead...      2                1\n",
       "186  What was the reasoning for 'move down' being s...      2                3"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>The player doesn't push any boxes. Shouldn't p...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>What's the justification for not using 'move r...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>What is the reasoning for not using 'push up'?</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>What made the plan opt for 'move left' instead...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>What was the reasoning for 'move down' being s...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset Prediction using Fine-Tuned Model",
   "id": "ee1d2c07765960c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<font color='red'>TODO</font>",
   "id": "7ae0825a6e00fda6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import BartForSequenceClassification, BartTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import wandb\n",
    "import os\n",
    "import pandas as pd\n",
    "from utils import get_best_available_device"
   ],
   "id": "d40e1a7736973599"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 2,
   "source": [
    "# set the memory fraction to 0.0 to avoid OOM errors\n",
    "torch.mps.set_per_process_memory_fraction(0.0)\n",
    "\n",
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"] = \"zero-shot-classification\"\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\""
   ],
   "id": "bfb05df3d563dccb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 3,
   "source": [
    "# Step 1: Read the CSV file using Pandas\n",
    "df = pd.read_csv('./data/combined_dataset.csv')\n",
    "\n",
    "# Convert labels to start from 0\n",
    "df['label'] = df['label'] - 1"
   ],
   "id": "3d6dea662c05331b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stringifying the column:   0%|          | 0/346 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7fa1a758b2c0487caa167b0c33fd75a3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Casting to class labels:   0%|          | 0/346 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c058b4023e9040e8a3d1c7453b3b59c6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 346\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4,
   "source": [
    "# Step 2: Convert the Pandas DataFrame to a Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.class_encode_column(\"label\")\n",
    "dataset"
   ],
   "id": "ca1c269b701cc356"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 5,
   "source": [
    "# Step 3: Split the dataset into training and testing sets\n",
    "split = dataset.train_test_split(test_size=0.2, stratify_by_column=\"label\", seed=13)\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': split['train'],\n",
    "    'test': split['test']\n",
    "})"
   ],
   "id": "7ab9b90be9459db1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 6,
   "source": [
    "# Step 4: Set up the tokenizer and model\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "model = BartForSequenceClassification.from_pretrained('facebook/bart-large-mnli', num_labels=3)"
   ],
   "id": "2f42cb7abe405252"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 7,
   "source": [
    "# Tokenize the dataset\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)"
   ],
   "id": "746de593c61bbe7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/276 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7012eb68a48b41dcb9ebe7a87f6d7199"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/70 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "19290b376d3546d49fa2a91c5e29fe8c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8,
   "source": "encoded_dataset = dataset_dict.map(preprocess_function, batched=True)",
   "id": "13d17e3cc0eecd0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 9,
   "source": [
    "# Set format for PyTorch\n",
    "encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ],
   "id": "6c9825869c15ca0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 10,
   "source": [
    "# device = get_best_available_device()\n",
    "device = 'cpu'\n",
    "model.to(device)  # Move the model to device\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "f9e1af2727f1a9e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 11,
   "source": [
    "# Step 5: Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    report_to=\"wandb\",\n",
    "    eval_strategy='epoch',\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ],
   "id": "47599d906c98b71a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 12,
   "source": [
    "# Step 6: Define the compute_metrics function\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, pred, average='weighted')\n",
    "    acc = accuracy_score(labels, pred)\n",
    "    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}"
   ],
   "id": "991c5981c5d6487c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 13,
   "source": [
    "# Step 7: Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset['train'],\n",
    "    eval_dataset=encoded_dataset['test'],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "id": "2205c0524772ddbd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mg-nitin\u001B[0m (\u001B[33mniting\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/Users/nitingupta/usc/aiisc/planning_ontology/intent_parsing/wandb/run-20240603_122742-11pl99ih</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/niting/zero-shot-classification/runs/11pl99ih' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/niting/zero-shot-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/niting/zero-shot-classification' target=\"_blank\">https://wandb.ai/niting/zero-shot-classification</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/niting/zero-shot-classification/runs/11pl99ih' target=\"_blank\">https://wandb.ai/niting/zero-shot-classification/runs/11pl99ih</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/105 : < :, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null,
   "source": [
    "# Step 8: Train the model\n",
    "trainer.train()"
   ],
   "id": "22818fc283861055"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
