{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Sentence Similarity\n",
    "Sentence similarity involves converting sentences into high-dimensional vector representations and then calculating the similarity between these vectors. \n",
    "\n",
    "This is typically done using pre-trained models that generate sentence embeddings. These embeddings can be used to calculate the similarity between sentences using cosine similarity or other distance metrics."
   ],
   "id": "8164ed30d2ac61a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Single Sentence Example",
   "id": "321fda4a1acdd8ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T22:12:02.555820Z",
     "start_time": "2024-06-02T22:12:02.546853Z"
    }
   },
   "cell_type": "code",
   "source": "from sentence_transformers import SentenceTransformer, util",
   "id": "d672e1e36977bed1",
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-02T22:12:04.769253Z",
     "start_time": "2024-06-02T22:12:02.558592Z"
    }
   },
   "source": [
    "# Load an example pre-trained sentence transformer models\n",
    "model_miniLM = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nitingupta/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T22:12:04.935615Z",
     "start_time": "2024-06-02T22:12:04.770169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Intent questions for each category\n",
    "intents = {\n",
    "    1: [\"Why is action A not used in the plan?\"],\n",
    "    2: [\"Why is action A used in the plan?\"],\n",
    "    3: [\"Why is action A used rather than action B?\"]\n",
    "}\n",
    "\n",
    "# Encode example questions\n",
    "intent_embeddings_miniLM = {k: model_miniLM.encode(v) for k, v in intents.items()}"
   ],
   "id": "d08ee4abd79e1382",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T22:12:04.938239Z",
     "start_time": "2024-06-02T22:12:04.936280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def categorize_question_similarity(question, model, intent_embeddings):\n",
    "    question_embedding = model.encode(question)  # Encode the input question\n",
    "\n",
    "    # Compute similarity scores\n",
    "    scores = {k: util.pytorch_cos_sim(question_embedding, v).item() for k, v in intent_embeddings.items()}\n",
    "    \n",
    "    # Find the category with the highest similarity score\n",
    "    return max(scores, key=scores.get)"
   ],
   "id": "b9d8aab2e3147be8",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T22:12:04.953680Z",
     "start_time": "2024-06-02T22:12:04.939492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = \"Why is action A used rather than action B?\"\n",
    "category = categorize_question_similarity(question, model_miniLM, intent_embeddings_miniLM)\n",
    "print(f\"The question belongs to category: {category}\")"
   ],
   "id": "af5e498a9fe524f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question belongs to category: 3\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<br>",
   "id": "8b92c9bfcf9c38b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluate Different Models on the Dataset",
   "id": "4c492452f4fc935"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T22:12:04.972885Z",
     "start_time": "2024-06-02T22:12:04.954333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "df = pd.read_csv('./data/combined_dataset.csv')\n",
    "print(f\"Number of rows in the dataset: {df.shape[0]}\")\n",
    "df.head()"
   ],
   "id": "f83fb0d47f3b553d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the dataset: 346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                text  label\n",
       "0           Why was action A excluded from the plan?      1\n",
       "1  What were the reasons for omitting action A fr...      1\n",
       "2  Can you explain why action A was not considere...      1\n",
       "3              Why didn't the plan include action A?      1\n",
       "4  What is the rationale for not using action A i...      1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why was action A excluded from the plan?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What were the reasons for omitting action A fr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can you explain why action A was not considere...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why didn't the plan include action A?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the rationale for not using action A i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T22:12:04.978182Z",
     "start_time": "2024-06-02T22:12:04.973624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def text_similarity_from_model(model_name, df):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    intent_embeddings = {k: model.encode(v) for k, v in intents.items()}\n",
    "\n",
    "    # Apply the classification function to the dataset\n",
    "    df['predicted_label'] = df['text'].apply(categorize_question_similarity, model=model, intent_embeddings=intent_embeddings)\n",
    "    \n",
    "    # Evaluate the results\n",
    "    y_true = df['label']\n",
    "    y_pred = df['predicted_label']\n",
    "    \n",
    "    return classification_report(y_true, y_pred), accuracy_score(y_true, y_pred)"
   ],
   "id": "8722dce0e62d92a8",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T22:12:09.585355Z",
     "start_time": "2024-06-02T22:12:04.979010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class_report, accuracy = text_similarity_from_model(\"paraphrase-MiniLM-L6-v2\", df)\n",
    "print(class_report)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ],
   "id": "df34b03400059564",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nitingupta/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.90      0.81      0.85       116\n",
      "           2       0.52      1.00      0.68       115\n",
      "           3       1.00      0.17      0.28       115\n",
      "\n",
      "    accuracy                           0.66       346\n",
      "   macro avg       0.81      0.66      0.61       346\n",
      "weighted avg       0.81      0.66      0.61       346\n",
      "\n",
      "Accuracy: 0.66\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T22:12:15.493059Z",
     "start_time": "2024-06-02T22:12:09.586017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class_report, accuracy = text_similarity_from_model(\"all-MiniLM-L12-v2\", df)\n",
    "print(class_report)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ],
   "id": "3a8e1dbcbd1bcc7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nitingupta/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.87      0.79       116\n",
      "           2       0.65      0.83      0.73       115\n",
      "           3       0.83      0.43      0.56       115\n",
      "\n",
      "    accuracy                           0.71       346\n",
      "   macro avg       0.73      0.71      0.69       346\n",
      "weighted avg       0.73      0.71      0.69       346\n",
      "\n",
      "Accuracy: 0.71\n"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
