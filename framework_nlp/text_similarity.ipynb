{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Sentence Similarity\n",
    "Sentence similarity involves converting sentences into high-dimensional vector representations and then calculating the similarity between these vectors. \n",
    "\n",
    "This is typically done using pre-trained models that generate sentence embeddings. These embeddings can be used to calculate the similarity between sentences using cosine similarity or other distance metrics."
   ],
   "id": "8164ed30d2ac61a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T02:26:34.835134Z",
     "start_time": "2024-05-31T02:26:34.831784Z"
    }
   },
   "cell_type": "code",
   "source": "from sentence_transformers import SentenceTransformer, util",
   "id": "d672e1e36977bed1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-31T02:26:36.735541Z",
     "start_time": "2024-05-31T02:26:34.838592Z"
    }
   },
   "source": [
    "# Load a pre-trained sentence transformer models\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nitingupta/miniconda3/envs/auto-plan/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T02:26:37.270304Z",
     "start_time": "2024-05-31T02:26:36.741091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Intent questions for each category\n",
    "intents = {\n",
    "    1: [\"Why is action A not used in the plan?\"],\n",
    "    2: [\"Why is action A used in the plan?\"],\n",
    "    3: [\"Why is action A used rather than action B?\"]\n",
    "}\n",
    "\n",
    "# Encode example questions\n",
    "intent_embeddings = {k: model.encode(v) for k, v in intents.items()}"
   ],
   "id": "d08ee4abd79e1382",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T02:26:37.276741Z",
     "start_time": "2024-05-31T02:26:37.272408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def categorize_question_similarity(question):\n",
    "    question_embedding = model.encode(question)  # Encode the input question\n",
    "\n",
    "    # Compute similarity scores\n",
    "    scores = {k: util.pytorch_cos_sim(question_embedding, v).item() for k, v in intent_embeddings.items()}\n",
    "    \n",
    "    # Find the category with the highest similarity score\n",
    "    return max(scores, key=scores.get)"
   ],
   "id": "b9d8aab2e3147be8",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T02:26:37.311783Z",
     "start_time": "2024-05-31T02:26:37.280822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = \"Why is action A used rather than action B?\"\n",
    "category = categorize_question_similarity(question)\n",
    "print(f\"The question belongs to category: {category}\")"
   ],
   "id": "af5e498a9fe524f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question belongs to category: 3\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<br>",
   "id": "8b92c9bfcf9c38b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluate the Model on Dataset",
   "id": "4c492452f4fc935"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T02:30:14.465884Z",
     "start_time": "2024-05-31T02:30:14.441735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "df = pd.read_csv('./data/intent_classification_dataset.csv')\n",
    "print(f\"Number of rows in the dataset: {df.shape[0]}\")\n",
    "df.head()"
   ],
   "id": "902c2efe4067d516",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the dataset: 107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                text  label\n",
       "0  Why is action A not included in the project ro...      1\n",
       "1  What are the reasons for excluding action A fr...      1\n",
       "2        Why was action A omitted from the strategy?      1\n",
       "3  Why didn't we consider action A for the projec...      1\n",
       "4       Why was action A left out of the final plan?      1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why is action A not included in the project ro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the reasons for excluding action A fr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why was action A omitted from the strategy?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why didn't we consider action A for the projec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why was action A left out of the final plan?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T02:26:42.366705Z",
     "start_time": "2024-05-31T02:26:37.326015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply the classification function to the dataset\n",
    "df['predicted_label'] = df['text'].apply(categorize_question_similarity)\n",
    "\n",
    "# Evaluate the results\n",
    "y_true = df['label']\n",
    "y_pred = df['predicted_label']"
   ],
   "id": "2e7e75b711aec8c9",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T02:26:42.379004Z",
     "start_time": "2024-05-31T02:26:42.367536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print classification report\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_true, y_pred):.2f}\")"
   ],
   "id": "b848c8b72573fb2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.89      0.69      0.78        36\n",
      "           2       0.55      1.00      0.71        35\n",
      "           3       1.00      0.42      0.59        36\n",
      "\n",
      "    accuracy                           0.70       107\n",
      "   macro avg       0.81      0.70      0.69       107\n",
      "weighted avg       0.82      0.70      0.69       107\n",
      "\n",
      "Accuracy: 0.70\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
